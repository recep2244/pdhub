# Protein Design Hub - Default Configuration

# Output settings
output:
  base_dir: "./outputs"
  save_all_models: true
  generate_report: true

# Predictor settings
predictors:
  # ColabFold settings
  colabfold:
    enabled: true
    num_models: 5
    num_recycles: 3
    use_amber: false
    use_templates: false
    msa_mode: "mmseqs2_uniref_env"  # mmseqs2_uniref, mmseqs2_uniref_env, single_sequence

  # Chai-1 settings
  chai1:
    enabled: true
    num_trunk_recycles: 3
    num_diffusion_timesteps: 200
    seed: 42

  # Boltz-2 settings
  boltz2:
    enabled: true
    recycling_steps: 3
    sampling_steps: 200
    diffusion_samples: 1

# Evaluation settings
evaluation:
  metrics:
    - lddt
    - tm_score
    - qs_score
    - rmsd
  lddt:
    inclusion_radius: 15.0
    sequence_separation: 0
  tm_score:
    tmalign_path: null  # Auto-detect

# Installation settings
installation:
  auto_update: false
  check_updates_on_start: true
  colabfold_path: null  # Auto-detect
  tools_dir: "~/.protein_design_hub/tools"

# GPU settings
gpu:
  device: "cuda:0"
  clear_cache_between_jobs: true
  memory_fraction: 0.95

# LLM settings (for agent meetings)
# Set 'provider' and it auto-configures base_url/model/api_key.
# Leave base_url/model/api_key empty to use provider defaults.
#
# Local providers (no API key needed):
#   ollama     – http://localhost:11434  (default: llama3.2:latest)
#   lmstudio   – http://localhost:1234
#   vllm       – http://localhost:8000
#   llamacpp   – http://localhost:8080
#
# Cloud providers (API key via config or env var):
#   deepseek   – $0.28/1M tokens (DEEPSEEK_API_KEY)
#   openai     – gpt-4o (OPENAI_API_KEY)
#   gemini     – free tier available (GEMINI_API_KEY)
#   kimi       – kimi-k2 (MOONSHOT_API_KEY)
#   custom     – set base_url/model/api_key manually
llm:
  provider: "ollama"        # Change to deepseek, openai, gemini, etc.
  # base_url: ""            # Auto-set from provider
  # model: ""               # Auto-set from provider
  # api_key: ""             # Auto-set from provider or env var
  temperature: 0.2
  max_tokens: 4096
  num_rounds: 1             # Discussion rounds per meeting

# Web UI settings
web:
  host: "localhost"
  port: 8501
  theme: "light"
